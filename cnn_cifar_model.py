# -*- coding: utf-8 -*-
"""CNN-cifar model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aAoFmpEvhs2dMBR_Fu2V0y5wo_R1ASH3

```
CNN for computer vision
```
Task: Build and train a convolutional neural network to classify images of clothing using the CIFAR 10 dataset.

Part 2 and 3: CIFAR 10 Datasets with Simple CNNs

Install and import dependencies
"""

import tensorflow as tf
# Import TensorFlow Datasets
import tensorflow_datasets as tfds
tfds.disable_progress_bar()

# Helper libraries
import math
import numpy as np
import matplotlib.pyplot as plt

import logging
logger = tf.get_logger()
logger.setLevel(logging.ERROR)

"""
Extract and load the CIFAR dataset

"""

import os
import glob

# Download the data
_URL = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'
zip_dir = tf.keras.utils.get_file('cifar-10-python.tar.gz', origin=_URL, extract=True)

# Get the data and meta file names
data_dir = os.path.join(os.path.dirname(zip_dir), 'cifar-10-batches-py')
train_files = glob.glob(os.path.join(data_dir,"data_batch_*"))
test_file = os.path.join(data_dir,"test_batch")
meta_file = os.path.join(data_dir,"batches.meta")

def unpickle(file):
    import pickle
    with open(file, 'rb') as fo:
        dict = pickle.load(fo, encoding='bytes')
    return dict

def build_dataset(files):
    x = []
    y = []
    for file in files:
        dict = unpickle(file)
        for image in dict[b'data']:
            # Image in the dataset is stored as a 3072 length 1D array
            x.append(image)
        for label in dict[b'labels']:
            y.append(label)

    return tf.data.Dataset.from_tensor_slices((x,y))

# Build the training dataset
train_dataset  = build_dataset(train_files)

# Build the testing dataset
test_dataset = build_dataset([test_file])

# Get the metadata
meta = unpickle(meta_file)

"""Preprocess the data: normalization and reshaping"""

def reshape_and_normalize(images, labels):
    # Convert from 1D array to 3D array of [3, 32, 32]
    # the image is stored as [colour channel, width, height]
    images = tf.reshape(images, (3, 32, 32))
    # Swap from [colour channel, width, height] to [width, height, colour channel]
    images = tf.transpose(images, (1, 2, 0))
    # Convert to float32
    images = tf.cast(images, tf.float32)
    # Normalize
    images /= 255
    return images, labels


train_dataset =  train_dataset.map(reshape_and_normalize)
test_dataset  =  test_dataset.map(reshape_and_normalize)

num_train_examples = 50000
BATCH_SIZE = 32
train_dataset = train_dataset.cache().shuffle(num_train_examples).batch(BATCH_SIZE)
test_dataset = test_dataset.cache().batch(BATCH_SIZE)

"""Create and train model"""

model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,
                           input_shape=(32, 32, 3)),
    tf.keras.layers.MaxPool2D((2, 2), strides=2),
    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=tf.nn.relu),
    tf.keras.layers.MaxPool2D((2, 2), strides=2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation=tf.nn.relu),
    tf.keras.layers.Dense(10)
])

model.summary()

"""Add a few more dense layers on top"""

model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(10))

model.summary()

"""Compile the model"""

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy', 'mse'])
history = model.fit(train_dataset,
          epochs=10,
          validation_data=test_dataset)

"""Evaluate the model"""

model.evaluate(test_dataset)

plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.5, 1])
plt.legend(loc='lower right')

results = model.evaluate(x_test,  y_test, verbose=2)
test_loss, test_acc = results[0], results[1]

"""Accuracy:"""

print(test_acc)

"""Generate a confusion matrix:"""

from sklearn.metrics import confusion_matrix
import numpy as np

# Make predictions on the test data
y_pred = model.predict(x_test)

# Convert the predicted probabilities to class labels
y_pred_classes = np.argmax(y_pred, axis=1)

# Generate the confusion matrix
confusion_matrix = confusion_matrix(y_test, y_pred_classes)

# Print the confusion matrix
print(confusion_matrix)

"""Save the model"""

model_name = 'cifar_cnn.h5'
model.save(model_name, save_format='h5')